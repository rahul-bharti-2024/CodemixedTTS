{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul_b/miniconda3/envs/voicecraft/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "  0%|          | 0/3136 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LN 109 यहाँ हम अपने ऑपरेटिंग सिस्टम के रूप में gnu/linux और लिबरऑफिस वर्जन 334 का उपयोग कर रहे हैं लिबर ऑफिस impress में एक प्रस्तुति document बनाना और बुनियादी formatting के इस spoken tutorial में आपका स्वागत है\n",
      "og torch.Size([1, 400, 4])\n",
      "multi size torch.Size([1, 205]) torch.Size([1, 400, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3136 [00:09<8:26:11,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103085_w5Jyq3XMbb3WwiKQ_0000.wav\n",
      "Saved to  /mnt/LS226/LS25/rahul2022387/OpenSLR/Results_mucs/samples_enhprompts/103085_w5Jyq3XMbb3WwiKQ_0000.wav\n",
      "LN 109 यहाँ हम अपने ऑपरेटिंग सिस्टम के रूप में gnu/linux और लिबरऑफिस वर्जन 334 का उपयोग कर रहे हैं इस tutorial में हम impress window के भागों के बारे में सीखेंगे और कैसे स्लाइड इन्सर्ट करें और कॉपी करें\n",
      "og torch.Size([1, 400, 4])\n",
      "multi size torch.Size([1, 195]) torch.Size([1, 400, 4])\n",
      "LN 109 यहाँ हम अपने ऑपरेटिंग सिस्टम के रूप में gnu/linux और लिबरऑफिस वर्जन 334 का उपयोग कर रहे हैं फॉन्ट तथा फॉन्ट को फॉर्मेट करना सीखेंगे\n",
      "og torch.Size([1, 400, 4])\n",
      "multi size torch.Size([1, 131]) torch.Size([1, 400, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3136 [01:06<58:16:58, 66.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 195\u001b[0m\n\u001b[1;32m    190\u001b[0m     seed: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    194\u001b[0m args\u001b[39m=\u001b[39m Config()\n\u001b[0;32m--> 195\u001b[0m main(args)\n",
      "Cell \u001b[0;32mIn[21], line 167\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    165\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mfor\u001b[39;00m idx, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(test)):\n\u001b[0;32m--> 167\u001b[0m     process_item(args, model, config, phn2num, audio_tokenizer, item, output_dir, idx)\n",
      "Cell \u001b[0;32mIn[21], line 115\u001b[0m, in \u001b[0;36mprocess_item\u001b[0;34m(args, model, config, phn2num, audio_tokenizer, item, output_dir, i)\u001b[0m\n\u001b[1;32m    113\u001b[0m chunk_i \u001b[39m=\u001b[39m prompt \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m chunk\n\u001b[1;32m    114\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLN 109\u001b[39m\u001b[39m\"\u001b[39m, chunk_i)\n\u001b[0;32m--> 115\u001b[0m concated_audio, gen_audio \u001b[39m=\u001b[39m inference_one_sample_graphemes(\n\u001b[1;32m    116\u001b[0m     model, Namespace(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig), phn2num, audio_tokenizer, orig_audio, \n\u001b[1;32m    117\u001b[0m     chunk_i, device, decode_config, prompt_end_frame\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m audio_chunks\u001b[39m.\u001b[39mappend(gen_audio\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu())\n\u001b[1;32m    120\u001b[0m     \u001b[39m# print(gen_audio.shape)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# prompt = chunk\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m# except Exception as e:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m#     # print('skipped', e, item)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m#     print('skipped', e)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m#     return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/voicecraft/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/IndicVoices-R/VoiceCraft/inference_tts_scale.py:86\u001b[0m, in \u001b[0;36minference_one_sample_graphemes\u001b[0;34m(model, model_args, phn2num, audio_tokenizer, audio_fn, target_text, device, decode_config, prompt_end_frame)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmulti size\u001b[39m\u001b[39m'\u001b[39m, text_tokens\u001b[39m.\u001b[39mshape, original_audio[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,:model_args\u001b[39m.\u001b[39mn_codebooks]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     85\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrunning inference with batch size \u001b[39m\u001b[39m{\u001b[39;00mdecode_config[\u001b[39m'\u001b[39m\u001b[39msample_batch_size\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, i.e. return the shortest among \u001b[39m\u001b[39m{\u001b[39;00mdecode_config[\u001b[39m'\u001b[39m\u001b[39msample_batch_size\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m generations.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m     concat_frames, gen_frames \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minference_tts_batch(\n\u001b[1;32m     87\u001b[0m         text_tokens\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     88\u001b[0m         text_tokens_lens\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     89\u001b[0m         original_audio[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m,:model_args\u001b[39m.\u001b[39;49mn_codebooks]\u001b[39m.\u001b[39;49mto(device), \u001b[39m# [1,T,8]\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m         top_k\u001b[39m=\u001b[39;49mdecode_config[\u001b[39m'\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     91\u001b[0m         top_p\u001b[39m=\u001b[39;49mdecode_config[\u001b[39m'\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     92\u001b[0m         temperature\u001b[39m=\u001b[39;49mdecode_config[\u001b[39m'\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     93\u001b[0m         stop_repetition\u001b[39m=\u001b[39;49mdecode_config[\u001b[39m'\u001b[39;49m\u001b[39mstop_repetition\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     94\u001b[0m         kvcache\u001b[39m=\u001b[39;49mdecode_config[\u001b[39m'\u001b[39;49m\u001b[39mkvcache\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     95\u001b[0m         batch_size \u001b[39m=\u001b[39;49m decode_config[\u001b[39m'\u001b[39;49m\u001b[39msample_batch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     96\u001b[0m         silence_tokens\u001b[39m=\u001b[39;49m\u001b[39meval\u001b[39;49m(decode_config[\u001b[39m'\u001b[39;49m\u001b[39msilence_tokens\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39mif\u001b[39;49;00m \u001b[39mtype\u001b[39;49m(decode_config[\u001b[39m'\u001b[39;49m\u001b[39msilence_tokens\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m==\u001b[39;49m\u001b[39mstr\u001b[39;49m \u001b[39melse\u001b[39;49;00m decode_config[\u001b[39m'\u001b[39;49m\u001b[39msilence_tokens\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     97\u001b[0m     ) \u001b[39m# output is [1,K,T]\u001b[39;00m\n\u001b[1;32m     98\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minference on one sample take: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstime\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m sec.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgenerated encoded_frames.shape: \u001b[39m\u001b[39m{\u001b[39;00mgen_frames\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, which is \u001b[39m\u001b[39m{\u001b[39;00mgen_frames\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39mdecode_config[\u001b[39m'\u001b[39m\u001b[39mcodec_sr\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m sec.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/IndicVoices-R/VoiceCraft/models/voicecraft.py:1469\u001b[0m, in \u001b[0;36mVoiceCraft.inference_tts_batch\u001b[0;34m(self, x, x_lens, y, top_k, top_p, temperature, stop_repetition, kvcache, batch_size, silence_tokens, *kargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m y_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_positional_embedding(embedded_y) \u001b[39m# [B T D]\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \u001b[39m# make attention mask and padding mask\u001b[39;00m\n\u001b[0;32m-> 1469\u001b[0m y_attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtriu(torch\u001b[39m.\u001b[39;49mones(y_input\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], y_input\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]), diagonal\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mbool()\u001b[39m.\u001b[39mto(y\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1470\u001b[0m new_y_lens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([y_input\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]])\u001b[39m.\u001b[39mto(y\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mrepeat(batch_size)\n\u001b[1;32m   1471\u001b[0m y_padding_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((batch_size,new_y_lens[\u001b[39m0\u001b[39m]), \u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mto(y\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from argparse import Namespace, ArgumentParser\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from data.tokenizer import AudioTokenizer, TextTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from inference_tts_scale import inference_one_sample_graphemes\n",
    "from shutil import copy2\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "def read_jsonl(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def get_spk2item(items):\n",
    "    spk2item = {}\n",
    "    for item in tqdm(items, desc='making spk2item'):\n",
    "        spk_id = item['speaker_id']\n",
    "        if spk_id not in spk2item.keys():\n",
    "            spk2item[spk_id] = [item]\n",
    "        else:\n",
    "            spk2item[spk_id].append(item)\n",
    "    return spk2item\n",
    "\n",
    "def contains_english_characters(sentence):\n",
    "    # Loop through each character in the sentence\n",
    "    for char in sentence:\n",
    "        # Check if the character is an English letter\n",
    "        if char.isalpha() and char.isascii():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def sample_speaker_prompt(spk2item, spk_id, item):\n",
    "    prompt_item = random.sample(spk2item[spk_id], 1)[0]\n",
    "    if prompt_item['text'] == item['text']: # make sure test set prompt not used\n",
    "        prompt_item = random.sample(spk2item[spk_id], 1)[0]\n",
    "    while prompt_item[\"duration\"] < 1:\n",
    "        prompt_item = random.sample(spk2item[spk_id], 1)[0]\n",
    "    return prompt_item\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def process_item(args, model, config, phn2num, audio_tokenizer, item, output_dir, i):\n",
    "    seed_everything(args.seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    orig_audio = item['prompt_audio']\n",
    "\n",
    "    # item['output_filename'] = f'{i}.wav'\n",
    "\n",
    "    # copy prompt to folder\n",
    "    prompt_dir = os.path.join(output_dir, 'prompts')\n",
    "    os.makedirs(prompt_dir, exist_ok=True)\n",
    "    dest_path = os.path.join(prompt_dir, item['output_filename'])\n",
    "    # try:\n",
    "    #     copy2(orig_audio, dest_path)\n",
    "    # except PermissionError:\n",
    "    #     from shutil import copy\n",
    "    #     copy(orig_audio, dest_path)\n",
    "\n",
    "    orig_transcript = item['prompt_text']\n",
    "    # test sentence\n",
    "\n",
    "    sentence = item['text']\n",
    "    item['text'] = sentence\n",
    "\n",
    "    filepath = f\"{output_dir}/{os.path.basename(orig_audio)[:-4]}.wav\"\n",
    "    # cut_off_sec = item['prompt_duration'] - 0.01\n",
    "    # target_transcript = orig_transcript + item['verbatim']\n",
    "    words = item['text'].split(\" \")\n",
    "    chunks = []\n",
    "    N_WORDS = 20 \n",
    "    for ix in range(0, len(words), N_WORDS):\n",
    "        \n",
    "        chunks.append(\" \".join(words[ix: ix+N_WORDS]))\n",
    "    chunks = list(filter(lambda x: x != \"\", chunks))\n",
    "    # print('info', orig_audio)\n",
    "    info = torchaudio.info(orig_audio)\n",
    "    audio_dur = info.num_frames / info.sample_rate\n",
    "    cut_off_sec = audio_dur - 0.01\n",
    "    # print(audio_dur)\n",
    "    # print('done info')\n",
    "\n",
    "    assert cut_off_sec < audio_dur, f\"cut_off_sec {cut_off_sec} is larger than the audio duration {audio_dur}\"\n",
    "    prompt_end_frame = int(cut_off_sec * info.sample_rate)\n",
    "\n",
    "    decode_config = {\n",
    "        'top_k': args.top_k, 'top_p': args.top_p, 'temperature': args.temperature, \n",
    "        'stop_repetition': args.stop_repetition, 'kvcache': args.kvcache, \"codec_audio_sr\": args.codec_audio_sr, \n",
    "        \"codec_sr\": args.codec_sr, \"silence_tokens\": args.silence_tokens, \"sample_batch_size\": args.sample_batch_size\n",
    "    }\n",
    "    prompt = orig_transcript\n",
    "    audio_chunks = []\n",
    "    for ix, chunk in enumerate(chunks):\n",
    "        chunk_i = prompt + ' ' + chunk\n",
    "        print(\"LN 109\", chunk_i)\n",
    "        concated_audio, gen_audio = inference_one_sample_graphemes(\n",
    "            model, Namespace(**config), phn2num, audio_tokenizer, orig_audio, \n",
    "            chunk_i, device, decode_config, prompt_end_frame\n",
    "        )\n",
    "        audio_chunks.append(gen_audio.squeeze(0).cpu())\n",
    "            # print(gen_audio.shape)\n",
    "            # prompt = chunk\n",
    "        # except Exception as e:\n",
    "        #     # print('skipped', e, item)\n",
    "        #     print('skipped', e)\n",
    "        #     return\n",
    "\n",
    "    gen_audio = torch.cat(audio_chunks, dim=1)\n",
    "    \n",
    "    # concated_audio, gen_audio = concated_audio[0].cpu(), gen_audio[0].cpu()\n",
    "    # filename = f'{i+1}_' + os.path.basename(orig_audio)\n",
    "\n",
    "    filename = item['output_filename']\n",
    "    print(filename)\n",
    "    samples_dir = os.path.join(output_dir, 'samples_enhprompts')\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    filepath = f\"{samples_dir}/{filename}\"\n",
    "    torchaudio.save(filepath, gen_audio, args.codec_audio_sr)\n",
    "    print('Saved to ', filepath)\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    from models import voicecraft\n",
    "    model_path = args.model_path\n",
    "\n",
    "    ckpt = torch.load(model_path, map_location='cpu')\n",
    "    model = voicecraft.VoiceCraft(ckpt['config'])\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    config = vars(model.args)\n",
    "    phn2num = ckpt[\"phn2num\"]\n",
    "\n",
    "    encodec_fn = \"./pretrained_models/encodec_4cb2048_giga.th\"\n",
    "    if not os.path.exists(encodec_fn):\n",
    "        os.system(f\"wget https://huggingface.co/pyp1/VoiceCraft/resolve/main/encodec_4cb2048_giga.th\")\n",
    "        os.system(f\"mv encodec_4cb2048_giga.th ./pretrained_models/encodec_4cb2048_giga.th\")\n",
    "    audio_tokenizer = AudioTokenizer(signature=encodec_fn, device=device)\n",
    "\n",
    "    filepath = args.manifest_path # '/nlsasfs/home/ai4bharat/praveens/ttsteam/repos/voicecraft/demo/srvm/demo_sys/demo.json'\n",
    "    test = read_jsonl(filepath)\n",
    "\n",
    "    output_dir = os.path.join(args.output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for idx, item in enumerate(tqdm(test)):\n",
    "        process_item(args, model, config, phn2num, audio_tokenizer, item, output_dir, idx)\n",
    "     \n",
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    manifest_path: str = '/home/rahul_b/IndicVoices-R/VoiceCraft/datasets/mucs/manifests/metadata_mucs_test2.jsonl'\n",
    "    model_path: str = '/home/rahul_b/IndicVoices-R/VoiceCraft/logs/mucs/e830M/best_bundle.pth'\n",
    "    output_dir: str = '/mnt/LS226/LS25/rahul2022387/OpenSLR/Results_mucs'\n",
    "    language_family: str = 'indoaryan'\n",
    "    language: str = ''\n",
    "    split: str = ''\n",
    "    replace_path: bool = False\n",
    "    num_workers: int = 1\n",
    "    codec_audio_sr: int = 16000\n",
    "    codec_sr: int = 50\n",
    "    top_k: int = 0\n",
    "    top_p: float = 0.9\n",
    "    temperature: float = 1.0\n",
    "    silence_tokens: List[int] = field(default_factory=lambda: [1388, 1898, 131])\n",
    "    kvcache: int = 1\n",
    "    stop_repetition: int = 4\n",
    "    sample_batch_size: int = 3\n",
    "    seed: int = 1\n",
    "\n",
    "\n",
    "\n",
    "args= Config()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.21 ('voicecraft': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d35e4ca48b9659a853600be3908db90ab4dc6a32ef8d2a8857bdbd9816fce733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
